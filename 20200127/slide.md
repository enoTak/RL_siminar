<!-- slide -->
# 20200127勉強会

## Deep Q Network (DQN)

<!-- slide -->

### Pendulum v0で実験してみよう

|設定  |概要  |範囲  |
|---|---|---|
|状態($s$)  |振り子角度・角速度  |3-dim  |
|行動($a$)  |振り子に作用する力 |-2~2 |
|終了条件 |最大ステップ数に到達したか |True/False|

* 状態: $s=(\cos \theta, \sin \theta, \dot \theta)$
* 報酬: $r=\theta^2 + 0.1 \dot \theta^2 + 0.001 a^2$

<!-- slide -->
## DQNアルゴリズム

<!-- slide -->
### どんなアルゴリズム?

* Q学習 + Q関数をDNNで近似

* ここまでのQ関数: 離散インプット→テーブルで関数データを持つ
・・・連続インプット難しかった

* 画像やテキスト: 状態としては扱い難しい
→DNNにより直接入力値にできる

* 当然計算量莫大になるので工夫する

<!-- slide -->
### 工夫1: ネットワーク構造

* よくあるやり方
  * 関数$(s, a) \mapsto Q_t(s, a)$を学習
  * 要は各$(s, a)$毎の値を学習
  * このあとやりたい計算は$\max_a Q_t(s, a)$なので各$a$毎に近似するのはコスト大
* なので関数$s \mapsto (Q_t(s, a))_{a \in A}$を学習する!
* $s \mapsto \max_a Q_t(s, a)$を学習すればメモリ削減できそうだが...?
→$\arg \max Q_t(s, a)$とか考えたいからNG

<!-- slide -->
### 工夫2: Experience replay

* パラメータ更新の際に生じる相関・バイアスをのぞく
* $(s, a, r, s')$の系列は$s$と相関が強い
  $s'$と$s$は近い状態にあるので
* $(s, a, r, s')$の過去データをプールしておき,
  パラメータ更新時はプールしてあるデータからサンプリングして利用
* 相関の少ないデータでの更新になるので、学習が収束しやすい

#### Rem: 方策オフ型になってしまう

<!-- slide -->
### 工夫3: Reward clipping

* Q関数の推定に報酬の値が影響する
* たまたま大きな報酬値が入ってしまうと、それに引きずられてしまう(収束が遅くなる)
* 成功したら報酬1, しなかったら報酬0と丸めてしまう

<!-- slide -->
### 工夫4: Double network

* Q学習でのQ関数の目標値:
 $q_*(s, a)$の代わりに$\max_{a \ \in A(s)} Q_t(s, a)$
* $Q_t(s, a)$の上方向の誤差影響が出やすい
  (目標値が高めに見積もられる)

* 行動選択のためのネットワーク(main)とQ関数計算のためのネットワーク(target)を別にする方法

* TD誤差計算において$$a_{t+1} = \arg \max_{a'} Q_{main}(s_{t+1}, a'), \\
target = R_{t+1} + \gamma Q_{target}(s_{t+1}, a_{t+1})$$で計算
* targetをmainと定期的に同期する

<!-- slide -->
### 全体でのアルゴリズム

#### 価値ベース

* 以下をループ:

  1. 今の状態から行動選択
  1. 次の状態と報酬を取得/報酬はreward clipping
  1. 経験をメモリに保存
  1. メモリからサンプリング
  1. サンプリング値を使ってネットワーク更新

<!-- slide -->
## 実装例(DQN)

<!-- slide -->
## Actor-Critic法の応用

<!-- slide -->
### Actor

* 行動変数$a$を2値化
* policyは$$\pi (a | s, \theta) = \frac{\exp\xi(s, a| \theta)}{\sum_{a'} \exp\xi(s, a'| \theta)}$$でモデル化
* $xi$をNNで推定

<!-- slide -->
### Critic

* Criticはadvantage関数をActorに渡す
* advantage関数 $\approx$ TD誤差なので
  Criticは価値関数$V(s)$をモデル化すればよい

* 価値関数と方策関数の引数はどちらも同じ$s$なのでCriticの出力を両方($V, a_1, a_2 (a_1 + a_2 = 1)$)としてもよい

<!-- slide -->
## 実装例(DQN/Actor-Critic)
