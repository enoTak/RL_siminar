{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP setting\n",
    "* state: index of each arrays\n",
    "  * 0: Home, 1: Office, 2: Bar\n",
    "* MDP array describes the probabilities for move to next state if choiced moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = [0.8, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward $r(s'| s, a)$\n",
    "* 1st index: $s'$\n",
    "* 2nd index: $s$\n",
    "* 3rd index: $a = True(1) or False(0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = np.zeros((3, 3, 2))\n",
    "r[0, 1, 0] = 1.0\n",
    "r[0, 2, 0] = 2.0\n",
    "r[0, 0, 1] = 0.0\n",
    "r[1, 0, 0] = 1.0\n",
    "r[1, 2, 0] = 2.0\n",
    "r[1, 1, 1] = 1.0\n",
    "r[2, 0, 0] = 1.0\n",
    "r[2, 1, 0] = 0.0\n",
    "r[2, 2, 1] = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Value Function $v_{\\pi_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial action Value funcion $q_{\\pi_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = np.zeros((3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initail policy distribution $\\pi_0$\n",
    "* the array describes the probabilities for choice of move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi = [0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition of policy estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_estimator(pi, p, r, gamma):\n",
    "    # initialize\n",
    "    # R: reward expection conditioned by initial state\n",
    "    # P: transtion probability between the states\n",
    "    \n",
    "    R = [0, 0, 0]\n",
    "    P = np.zeros((3, 3))\n",
    "\n",
    "    for i in range(3):\n",
    "\n",
    "        # 状態遷移行列の計算\n",
    "        P[i, i] = 1 - pi[i]\n",
    "        P[i, (i + 1) % 3] = p[i] * pi[i]\n",
    "        P[i, (i + 2) % 3] = (1 - p[i]) * pi[i]\n",
    "\n",
    "        # 報酬ベクトルの計算\n",
    "        R[i] = pi[i] * (p[i] * r[i, (i + 1) % 3, 0] +\n",
    "                        (1 - p[i]) * r[i, (i + 2) % 3, 0]\n",
    "                        ) + (1 - pi[i]) * r[i, i, 1]\n",
    "\n",
    "    # 行列計算によるベルマン方程式の求解\n",
    "    A = np.eye(3) - gamma * P\n",
    "    B = np.linalg.inv(A)\n",
    "    v_sol = np.dot(B, R)\n",
    "\n",
    "    return v_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = [0, 0, 0]\n",
    "v_pev = copy.copy(v)\n",
    "\n",
    "for step in range(100):\n",
    "\n",
    "    # 方策評価ステップ\n",
    "    v = policy_estimator(pi, p, r, gamma)\n",
    "\n",
    "    # 価値関数 v が前ステップの値 v_prep を改善しなければ終了\n",
    "    if np.min(v - v_prev) <= 0:\n",
    "        break\n",
    "\n",
    "    # 現ステップの価値関数と方策を表示\n",
    "    print('step:', step, ' value:', v, ' policy:', pi)\n",
    "\n",
    "    # 方策改善ステップ\n",
    "    for i in range(3):\n",
    "\n",
    "        # 行動価値関数を計算\n",
    "        q[i, 0] = p[i] * (\n",
    "            r[i, (i + 1) % 3, 0] + gamma * v[(i + 1) % 3]\n",
    "        ) + (1 - p[i]) * (r[i, (i + 2) % 3, 0]\n",
    "                          + gamma * v[(i + 2) % 3])\n",
    "        q[i, 1] = r[i, i, 1] + gamma * v[i]\n",
    "\n",
    "        # 行動価値関数のもとで greedy に方策を改善\n",
    "        if q[i, 0] > q[i, 1]:\n",
    "            pi[i] = 1\n",
    "        elif q[i, 0] == q[i, 1]:\n",
    "            pi[i] = 0.5\n",
    "        else:\n",
    "            pi[i] = 0\n",
    "\n",
    "    # 現ステップの価値関数を記録\n",
    "    v_prev = copy.copy(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
