{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward = np.zeros((4, 4, 2))\n",
    "reward[0, 1, 0] = 1.0\n",
    "reward[0, 2, 0] = 2.0\n",
    "reward[0, 0, 1] = 0.0\n",
    "reward[1, 0, 0] = 1.0\n",
    "reward[1, 2, 0] = 2.0\n",
    "reward[1, 1, 1] = 1.0\n",
    "reward[2, 0, 0] = 1.0\n",
    "reward[2, 1, 0] = 0.0\n",
    "reward[2, 2, 1] = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP setting\n",
    "* state: index of visiting count to each place\n",
    "  * 0: Home, 1: Office, 2: Bar 3: End State\n",
    "* MDP array describes the probabilities for move to next state if choiced moving\n",
    "* if count of Home = $n$, transit to End State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_prob = [0.8, 0.5, 1.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_state = 3\n",
    "\n",
    "\n",
    "def get_next_state(current_place, current_count, next_prob_arr, unif, end_count):\n",
    "    next_prob = next_prob_arr[current_place]\n",
    "    \n",
    "    if current_count[0] == end_count:\n",
    "        next_place = end_state\n",
    "        current_count[end_state] += 1\n",
    "    elif unif <= next_prob:\n",
    "        next_place = (current_place + 1) % 3\n",
    "        current_count[next_place] += 1\n",
    "    else:\n",
    "        next_place = (current_place + 2) % 3\n",
    "        current_count[next_place] += 1\n",
    "\n",
    "    return next_place, current_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [0, 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_count = [0, 0, 0, 0]\n",
    "get_next_state(0, current_count, transition_prob, 0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* policy definition ... move: 0, stay: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "move = 0\n",
    "stay = 1\n",
    "def policy(state, p, unif):\n",
    "    move_prob = p[state]\n",
    "    if unif <= move_prob:\n",
    "        return move\n",
    "    else:\n",
    "        return stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi = [0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_state = 0\n",
    "end_count = 5\n",
    "max_time_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = init_state\n",
    "count = [0, 0, 0, 0]\n",
    "a = 0\n",
    "r = 0.0\n",
    "state_hist = []\n",
    "action_hist = []\n",
    "reward_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count: 1  current state: 0  action: 0  next state: 1  reward: 1.0\n",
      "[0, 1, 0, 0]\n",
      "total count: 1  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[0, 1, 0, 0]\n",
      "total count: 1  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[0, 1, 0, 0]\n",
      "total count: 1  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[0, 1, 0, 0]\n",
      "total count: 2  current state: 1  action: 0  next state: 2  reward: 2.0\n",
      "[0, 1, 1, 0]\n",
      "total count: 3  current state: 2  action: 0  next state: 0  reward: 1.0\n",
      "[1, 1, 1, 0]\n",
      "total count: 4  current state: 0  action: 0  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 4  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 5  current state: 1  action: 0  next state: 0  reward: 1.0\n",
      "[2, 2, 1, 0]\n",
      "total count: 5  current state: 0  action: 1  next state: 0  reward: 0.0\n",
      "[2, 2, 1, 0]\n",
      "total count: 5  current state: 0  action: 1  next state: 0  reward: 0.0\n",
      "[2, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, max_time_step):\n",
    "    unif = np.random.uniform()\n",
    "    a = policy(s, pi, unif)\n",
    "    \n",
    "    if a == move:\n",
    "        unif = np.random.uniform()\n",
    "        next_s, count = get_next_state(s, count, transition_prob, unif, end_count)\n",
    "    else:\n",
    "        next_s = s\n",
    "\n",
    "    if next_s == end_state:\n",
    "        break\n",
    "        \n",
    "    r = reward[s, next_s, a]\n",
    "    \n",
    "    state_hist.append(s)\n",
    "    action_hist.append(a)\n",
    "    reward_hist.append(r)\n",
    "    \n",
    "    print('total count:', sum(count), ' current state:', s, ' action:', a, ' next state:', next_s, ' reward:', r)\n",
    "    print(count)\n",
    "            \n",
    "    s = next_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, [0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_hist), state_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, [1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reward_hist), reward_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indicator(state, value):\n",
    "    return state == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(target_state, cummulative_reward, current_state, current_value, learning_rate):\n",
    "    update_value = cummulative_reward - current_value\n",
    "    return current_value + learning_rate * update_value * indicator(current_state, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ret <- 0\n",
    "ret <- R_T\n",
    "ret <- R_T-1 + g * R_T\n",
    "ret <- R_T-2 + g * (R_T-1 + g * R_T) = R_T-2 + g * R_T-1 + g^2 * R_T\n",
    ".\n",
    ".\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount = 0.9\n",
    "\n",
    "\n",
    "def cumulative_reward_monte_carlo(state_hist, reward_hist):\n",
    "    ret = 0.0\n",
    "    for s, r in zip(state_hist[::-1], reward_hist[::-1]):\n",
    "        ret = r + discount * ret\n",
    "    return ret\n",
    "\n",
    "\n",
    "def cumulative_reward_td(state_hist, reward_hist):\n",
    "    return reward_hist[0] + discount * state_hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7818951100000007"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_reward_monte_carlo(state_hist, reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0  cum_reward: 1.9  estimate: 0.019\n",
      "state: 1  cum_reward: 1.9  estimate: 0.019\n",
      "state: 1  cum_reward: 1.9  estimate: 0.019\n",
      "state: 1  cum_reward: 1.9  estimate: 0.019\n",
      "state: 1  cum_reward: 3.8  estimate: 0.019\n",
      "state: 2  cum_reward: 1.0  estimate: 0.019\n",
      "state: 0  cum_reward: 1.9  estimate: 0.03781\n",
      "state: 1  cum_reward: 1.9  estimate: 0.03781\n",
      "state: 1  cum_reward: 1.0  estimate: 0.03781\n",
      "state: 0  cum_reward: 0.0  estimate: 0.0374319\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-279151ad6a4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_hist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcum_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_hist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f2501c65bf9e>\u001b[0m in \u001b[0;36mcumulative_reward_td\u001b[0;34m(state_hist, reward_hist)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcumulative_reward_td\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_hist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mreward_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstate_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "r_hist = copy.copy(reward_hist)\n",
    "s_hist = copy.copy(state_hist)\n",
    "cum_func = cumulative_reward_td\n",
    "\n",
    "target_state = 0\n",
    "learning_rate = 0.01\n",
    "\n",
    "val = 0.0\n",
    "while len(r_hist) > 0:\n",
    "    cum = cum_func(s_hist, r_hist)\n",
    "    current_state = s_hist[0]\n",
    "    val = update(target_state, cum, current_state, val, learning_rate)\n",
    "    print('state:', current_state, ' cum_reward:', cum, ' estimate:', val)\n",
    "    \n",
    "    if len(r_hist) == 0:\n",
    "        break\n",
    "        \n",
    "    r_hist = r_hist[1:]\n",
    "    s_hist = s_hist[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.092365370433378904"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
