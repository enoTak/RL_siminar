{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward = np.zeros((4, 4, 2))\n",
    "reward[0, 1, 0] = 1.0\n",
    "reward[0, 2, 0] = 2.0\n",
    "reward[0, 0, 1] = 0.0\n",
    "reward[1, 0, 0] = 1.0\n",
    "reward[1, 2, 0] = 2.0\n",
    "reward[1, 1, 1] = 1.0\n",
    "reward[2, 0, 0] = 1.0\n",
    "reward[2, 1, 0] = 0.0\n",
    "reward[2, 2, 1] = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP setting\n",
    "* state: index of visiting count to each place\n",
    "  * 0: Home, 1: Office, 2: Bar 3: End State\n",
    "* MDP array describes the probabilities for move to next state if choiced moving\n",
    "* if count of Home = $n$, transit to End State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_prob = [0.8, 0.5, 1.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_state = 3\n",
    "\n",
    "\n",
    "def get_next_state(current_place, current_count, next_prob_arr, unif, end_count):\n",
    "    next_prob = next_prob_arr[current_place]\n",
    "    \n",
    "    if sum(current_count) == end_count:\n",
    "        next_place = end_state\n",
    "        current_count[end_state] += 1\n",
    "    elif unif <= next_prob:\n",
    "        next_place = (current_place + 1) % 3\n",
    "        current_count[next_place] += 1\n",
    "    else:\n",
    "        next_place = (current_place + 2) % 3\n",
    "        current_count[next_place] += 1\n",
    "\n",
    "    return next_place, current_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [0, 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_count = [0, 0, 0, 0]\n",
    "get_next_state(0, current_count, transition_prob, 0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* policy definition ... move: 0, stay: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "move = 0\n",
    "stay = 1\n",
    "def policy(state, p, unif):\n",
    "    move_prob = p[state]\n",
    "    if unif <= move_prob:\n",
    "        return move\n",
    "    else:\n",
    "        return stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi = [0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_state = 0\n",
    "end_count = 5\n",
    "max_time_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = init_state\n",
    "count = [0, 0, 0, 0]\n",
    "a = 0\n",
    "r = 0.0\n",
    "state_hist = []\n",
    "action_hist = []\n",
    "reward_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count: 1  current state: 0  action: 0  next state: 1  reward: 1.0\n",
      "[0, 1, 0, 0]\n",
      "total count: 2  current state: 1  action: 0  next state: 2  reward: 2.0\n",
      "[0, 1, 1, 0]\n",
      "total count: 3  current state: 2  action: 0  next state: 0  reward: 1.0\n",
      "[1, 1, 1, 0]\n",
      "total count: 3  current state: 0  action: 1  next state: 0  reward: 0.0\n",
      "[1, 1, 1, 0]\n",
      "total count: 4  current state: 0  action: 0  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 4  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 4  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 4  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 4  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 4  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 4  current state: 1  action: 1  next state: 1  reward: 1.0\n",
      "[1, 2, 1, 0]\n",
      "total count: 5  current state: 1  action: 0  next state: 2  reward: 2.0\n",
      "[1, 2, 2, 0]\n",
      "total count: 5  current state: 2  action: 1  next state: 2  reward: -1.0\n",
      "[1, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, max_time_step):\n",
    "    unif = np.random.uniform()\n",
    "    a = policy(s, pi, unif)\n",
    "    \n",
    "    if a == move:\n",
    "        unif = np.random.uniform()\n",
    "        next_s, count = get_next_state(s, count, transition_prob, unif, end_count)\n",
    "    else:\n",
    "        next_s = s\n",
    "\n",
    "    if next_s == end_state:\n",
    "        break\n",
    "        \n",
    "    r = reward[s, next_s, a]\n",
    "    \n",
    "    state_hist.append(s)\n",
    "    action_hist.append(a)\n",
    "    reward_hist.append(r)\n",
    "    \n",
    "    print('total count:', sum(count), ' current state:', s, ' action:', a, ' next state:', next_s, ' reward:', r)\n",
    "    print(count)\n",
    "            \n",
    "    s = next_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, [0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_hist), state_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, [1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reward_hist), reward_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indicator(state, value):\n",
    "    return state == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(target_state, cummulative_reward, current_state, current_value, learning_rate):\n",
    "    update_value = cummulative_reward - current_value\n",
    "    return current_value + learning_rate * update_value * indicator(current_state, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount = 0.9\n",
    "\n",
    "\n",
    "def cumulative_reward_monte_carlo(state_hist, reward_hist):\n",
    "    ret = 0.0\n",
    "    for s, r in zip(state_hist[::-1], reward_hist[::-1]):\n",
    "        ret = r + discount * ret\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3780856947990001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_reward_monte_carlo(state_hist, reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0  cum_reward, 7.3780856948\n",
      "state: 1  cum_reward, 7.08676188311\n",
      "state: 2  cum_reward, 5.6519576479\n",
      "state: 0  cum_reward, 5.168841831\n",
      "state: 0  cum_reward, 5.74315759\n",
      "state: 1  cum_reward, 5.2701751\n",
      "state: 1  cum_reward, 4.744639\n",
      "state: 1  cum_reward, 4.16071\n",
      "state: 1  cum_reward, 3.5119\n",
      "state: 1  cum_reward, 2.791\n",
      "state: 1  cum_reward, 1.99\n",
      "state: 1  cum_reward, 1.1\n",
      "state: 2  cum_reward, -1.0\n"
     ]
    }
   ],
   "source": [
    "r_hist = copy.copy(reward_hist)\n",
    "s_hist = copy.copy(state_hist)\n",
    "cum_func = cumulative_reward_monte_carlo\n",
    "\n",
    "target_state = 0\n",
    "learning_rate = 0.01\n",
    "\n",
    "val = 0.0\n",
    "while len(r_hist) > 0:\n",
    "    cum = cum_func(s_hist, r_hist)\n",
    "    current_state = s_hist[0]\n",
    "    print('state:', current_state, ' cum_reward,', cum)\n",
    "    val = update(target_state, cum, current_state, val, learning_rate)\n",
    "    r_hist = r_hist[1:]\n",
    "    s_hist = s_hist[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04595438071421"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
